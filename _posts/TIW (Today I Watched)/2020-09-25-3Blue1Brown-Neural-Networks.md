---
title:  "3Blue1Brown Youtube Series on Neural Networks."
categories: TIW DataScience
author: Manuel
toc: true
toc_sticky: true
---

The great animated series from 3Blue1Brown on how neural networks actually work.

# But what is a Neural Network? | Deep learning, chapter 1

<iframe width="560" height="315" src="https://www.youtube.com/embed/aircAruvnKk" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
 <br>
Timeline:  
- 0:00 - Introduction example  
- 1:07 - Series preview  
- 2:42 - What are neurons?  
- 3:35 - Introducing layers  
- 5:31 - Why layers?  
- 8:38 - Edge detection example  
- 11:34 - Counting weights and biases  
- 12:30 - How learning relates  
- 13:26 - Notation and linear algebra  
- 15:17 - Recap  
- 16:27 - Some final words  
- 17:03 - ReLU vs Sigmoid  


# Gradient descent, how neural networks learn | Deep learning, chapter 2

<iframe width="560" height="315" src="https://www.youtube.com/embed/IHZwWFHWa-w" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<br>
Video timeline  
- 0:00 - Introduction  
- 0:30 - Recap  
- 1:49 - Using training data  
- 3:01 - Cost functions  
- 6:55 - Gradient descent  
- 11:18 - More on gradient vectors  
- 12:19 - Gradient descent recap  
- 13:01 - Analyzing the network  
- 16:37 - Learning more  
- 17:38 - Lisha Li interview  
- 19:58 - Closing thoughts  


## What is backpropagation really doing? | Deep learning, chapter 3

<iframe width="560" height="315" src="https://www.youtube.com/embed/Ilg3gGewQ5U" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<br>
Video timeline:  
- 0:00 - Introduction  
- 0:23 - Recap  
- 3:07 - Intuitive walkthrough example  
- 9:33 - Stochastic gradient descent  
- 12:28 - Final words

## Backpropagation calculus | Deep learning, chapter 4

<iframe width="560" height="315" src="https://www.youtube.com/embed/tIeHLnjs5U8" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<br>
Video timeline  
- 0:00 - Introduction  
- 0:38 - The Chain Rule in networks  
- 3:56 - Computing relevant derivatives  
- 4:45 - What do the derivatives mean?  
- 5:39 - Sensitivity to weights/biases  
- 6:42 - Layers with additional neurons  
- 9:13 - Recap
